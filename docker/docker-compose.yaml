version: "3.9"

volumes:
  kafka-data:
    driver: local

services:

  # Kafka
  kafka:
      image: confluentinc/cp-kafka:8.0.0
      hostname: kafka
      container_name: kafka
      restart: unless-stopped
      ports:
        - "9092:9092"
        - "9091:9091"
        # - "1234:1234"   # JMX Prometheus agent (disabilitato)
      volumes:
        - kafka-data:/var/lib/kafka/data:Z
        # - ./kafka/prometheus:/opt/prometheus:Z
      environment:

        # Configurazione di un processo kafka con il ruolo di
        # broker e controller. Si definiscono due listeners,
        # uno per parlare con gli altri broker (PLAINTEXT),
        # l'altro per parlare con altri controller (CONTROLLER)
        KAFKA_NODE_ID: 1
        KAFKA_PROCESS_ROLES: broker,controller
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
        KAFKA_LISTENERS: CONTROLLER://kafka:29093,PLAINTEXT://kafka:29092,PLAINTEXT_HOST://0.0.0.0:9092
        KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
        KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
        KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092

        # Dato un topic, questo può avere da 1 ad N partizioni
        # Ogni partizione è il contenitore logico dei messaggi
        # inviati al topic. I messaggi sono salvati in formato
        # compresso utilizzando gzip.
        KAFKA_NUM_PARTITIONS: 12
        KAFKA_COMPRESSION_TYPE: gzip
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
        KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1

        # KAFKA_LOG_DIRS:                  '/tmp/kraft-combined-logs'
        # KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
        CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk

        # JMX & Prometheus settings
        KAFKA_JMX_PORT: 9091
        KAFKA_JMX_HOSTNAME: kafka
        # KAFKA_JMX_OPTS: |
        #   -Dcom.sun.management.jmxremote=true 
        #   -Dcom.sun.management.jmxremote.rmi.port=9091 
        #   -Dcom.sun.management.jmxremote.authenticate=false 
        #   -Dcom.sun.management.jmxremote.ssl=false 
        #   -Dcom.sun.management.jmxremote.local.only=false 
        #   -javaagent:/usr/share/java/jmx_prometheus_javaagent.jar=1234:/opt/prometheus/config.yaml

  # Servizio HTTP che salva gli schema Avro/JSON/Protobuf in Kafka
  # Si collega al cluster Kakfa come client ed espone un listeners
  # a tutto il mondo sulla porta 8085
  schema-registry:
    image: confluentinc/cp-schema-registry:8.0.0
    hostname: schema-registry
    container_name: schema-registry
    restart: on-failure
    ports:
      - "8085:8085"
    depends_on:
      - kafka
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8085
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: INFO

  # Kakfa connect è un servizio che consente di eseguire dei connettori
  # ossia, delle procedure standard per trasferire dati da/verso Kakfa
  # in modo trasparente. Anch'esso è un client del cluster Kafka, si
  # collega ai broker ed espone un listener sulla porta 8083
  connect:
    image: cnfldemos/kafka-connect-datagen:0.6.7-8.0.0
    hostname: connect
    container_name: connect
    restart: on-failure
    depends_on:
      - kafka
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:29092
      CONNECT_REST_PORT: 8083
      CONNECT_REST_LISTENERS: http://0.0.0.0:8083
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: __connect-config
      CONNECT_OFFSET_STORAGE_TOPIC: __connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: __connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8085
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components

  akhq:
    image: tchiotludo/akhq:latest
    container_name: akhq
    restart: unless-stopped
    ports:
      - "8080:8080"
    depends_on:
      - kafka
      - schema-registry
      - connect
    environment:
      # AGENTS_URL: http://agents:8000/agents   # agents non definito
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            local-cluster:
              properties:
                bootstrap.servers: kafka:29092
              schema-registry:
                url: http://schema-registry:8085
              connect:
                - name: connect
                  url: http://connect:8083
              # ksqldb:
              #   - name: ksqldb
              #     url: http://ksqldb-server:8088
        # copilotkit:
        #   url: http://localhost:3001

  # Optional Agents service (commented out as per original)
  # agents:
  #   build:
  #     context: ../agents
  #   ports:
  #     - "8000:8000"
  #   env_file:
  #     - ../agents/.env

  # Grafana permette di creare dashboard interattive per 
  # la visualizzazione di metriche in tempo reale. Per 
  # funzionare, Grafana deve essere configurata con una 
  # o più sorgenti dati, ovvero servizi che raccolgono 
  # ed espongono le metriche da visualizzare. Una delle 
  # sorgenti dati più comuni è Prometheus.
  #
  # Prometheus consente di estrapolare metriche per tutti
  # i servizi che espongono delle metriche. La lista
  # completa dei componenti che offrono integrazione con
  # questo strumento è disponibile qui: 
  # https://prometheus.io/docs/instrumenting/exporters/
  # 
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: on-failure
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus:/etc/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'

  # Dalla pagina: https://github.com/danielqsj/kafka_exporter
  # questo strumento consente l'export di metriche da un cluster
  # Kakfa, in particolare dai processi broker
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: kafka-exporter
    restart: on-failure
    ports:
      - "9308:9308"
    depends_on:
      - kafka
    command:
      - '--kafka.server=kafka:29092'

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: on-failure
    # restart: always
    ports:
      - "3000:3000"
    # volumes:
    #   - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
    #   - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
    #   - ./grafana/dashboards:/etc/grafana/dashboards
    #   - ./grafana/grafana.ini:/etc/grafana/grafana.ini
    depends_on:
      - prometheus
    # environment:
    #   GF_AUTH_PROXY_ENABLED:         "true"
    #   GF_AUTH_PROXY_HEADER_NAME:     "X-WEBAUTH-USER"
    #   GF_AUTH_PROXY_HEADER_PROPERTY: "username"
    #   GF_AUTH_PROXY_AUTO_SIGN_UP:    "true"
